{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "from skimage.morphology import label\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the watershed shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology, segmentation\n",
    "\n",
    "def label_clusters(mat):\n",
    "    \"\"\"Sequentially labels distinct clusters in segmentation\n",
    "\n",
    "    The input can be either a True/False (foreground/background) segmentation,\n",
    "    or a matrix where every group/cluster shares the same value.\n",
    "    This will essentially relabel all the clusters starting from 0.\n",
    "    If a 3D array is passed in it will label each channel separately.\n",
    "\n",
    "    Args:\n",
    "        mat (ndarray): 2D/3D array with raw segmentation (or could be labeled)\n",
    "\n",
    "    Returns:\n",
    "        ndarray: 2D/3D array with labels for each cluster\n",
    "    \"\"\"\n",
    "    if len(mat.shape) == 3:\n",
    "        results = np.zeros_like(mat)\n",
    "        for c in tqdm(range(mat.shape[2])):\n",
    "            results[:, :, c] = label_clusters(mat[:, :, c])\n",
    "        return results\n",
    "    else:\n",
    "        return morphology.label(mat, connectivity=1, background=-1)\n",
    "\n",
    "\n",
    "def _label_graph(mat):\n",
    "    \"\"\"Labels segments, similar to label_clusters.\n",
    "\n",
    "    Like label_clusters, this labels each cluster in a segmentation starting from 0.\n",
    "    This uses a graph, and labels based on connected components.\n",
    "    It also returns the counts for each component for use to remove small groups.\n",
    "\n",
    "    Args:\n",
    "        mat (ndarray): a 2d matrix to label clusters\n",
    "\n",
    "    Returns:\n",
    "        tuple: the segmentation map, and the counts for each segment\n",
    "    \"\"\"\n",
    "    seg = label_clusters(mat)\n",
    "    counts = np.bincount(seg.flatten())\n",
    "    return seg, counts\n",
    "\n",
    "def _remove_singletons(seg, min_size, relabel=True):\n",
    "    \"\"\"Given a segmentaiton, it removes merges small clusters.\n",
    "\n",
    "    Any segment smaller than min_size will get merged with the neighbor with the most touching edges.\n",
    "\n",
    "    Args:\n",
    "        seg (ndarray): 2d array of labeled segments.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: relabeled segmentation with small clusters merged with nearby clusters.\n",
    "    \"\"\"\n",
    "    if relabel:\n",
    "        seg, counts = _label_graph(seg)\n",
    "        for i in range(len(counts)):\n",
    "            if counts[i] < min_size:\n",
    "                temp = seg == i\n",
    "                neigh = seg[segmentation.find_boundaries(temp, 2, 'outer')]\n",
    "                new_lab = np.bincount(neigh).argmax()\n",
    "                seg[seg == i] = new_lab\n",
    "        return label_clusters(seg)\n",
    "    else:\n",
    "        for i in np.unique(seg):\n",
    "            temp = seg == i\n",
    "            if np.sum(temp) < min_size:\n",
    "                neigh = seg[segmentation.find_boundaries(temp, 2, 'outer')]\n",
    "                new_lab = np.bincount(neigh).argmax()\n",
    "                seg[seg == i] = new_lab\n",
    "        new_lab = 1\n",
    "        new_seg = seg.copy()\n",
    "        for i in np.unique(seg):\n",
    "            new_seg[seg == i] = new_lab\n",
    "            new_lab += 1\n",
    "        return new_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in watersheds\n",
    "shapefile_path = \"/home/ScoutJarman/Code/ILWA/data/shapefiles/id_ut8_1601_exp/id_ut8_1601_exp.shp\"\n",
    "watersheds = gpd.read_file(shapefile_path)\n",
    "\n",
    "# get raster meta data information\n",
    "raster_path = \"/home/ScoutJarman/Code/ILWA/data/rasters/PRISM/ppt_81-23/ppt19810101.tif\"\n",
    "with rasterio.open(raster_path) as handle:\n",
    "    raster_arr = handle.read(1)\n",
    "    meta = handle.meta\n",
    "\n",
    "# Make segmentations\n",
    "seg = np.zeros_like(raster_arr)\n",
    "for idx, geometry in enumerate(watersheds['geometry']):\n",
    "    mask = geometry_mask([geometry], out_shape=raster_arr.shape, transform=meta['transform'], invert=True)\n",
    "    seg[mask] = idx + 1\n",
    "\n",
    "seg = seg.astype(int)\n",
    "seg = _remove_singletons(seg, 3, False)\n",
    "\n",
    "plt.imshow(seg, cmap='inferno')\n",
    "np.unique(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each ppt file, and aggregate it according to the watersheds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_in = [\"/home/ScoutJarman/Code/ILWA/data/rasters/PRISM/ppt_81-23\",\n",
    "              \"/home/ScoutJarman/Code/ILWA/data/rasters/PRISM/tmin_81_23\",\n",
    "              \"/home/ScoutJarman/Code/ILWA/data/rasters/PRISM/tmax_81_23\",\n",
    "              \"/home/ScoutJarman/Code/ILWA/data/rasters/UA/swe_81-23\"]\n",
    "var_names = ['ppt', 'tmin', 'tmax', 'swe']\n",
    "input_path = \"/home/ScoutJarman/Code/ILWA/data/rasters/UA/ua_swe_daily_bearlake.tif\"\n",
    "\n",
    "with open(\"/home/ScoutJarman/Code/ILWA/data/rasters/UA/ua_swe_daily_bearlake.tif.aux.json\", 'r') as json_file:\n",
    "    layer_names = json.load(json_file)['time']\n",
    "parsed_dates = np.asarray([datetime.strptime(date_str, \"%Y-%m-%d\") for date_str in layer_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "dates = []\n",
    "pbar =  tqdm(os.listdir(folders_in[0]))\n",
    "for file in pbar:\n",
    "    year = int(file[3:7])\n",
    "    month = int(file[7:9])\n",
    "    day = int(file[9:11])\n",
    "    nums = file[3:-4]\n",
    "    dtime = datetime(year, month, day)\n",
    "\n",
    "    try:\n",
    "        tmp_data = []\n",
    "        for i, f_in in enumerate(folders_in):\n",
    "            with rasterio.open(os.path.join(f_in, f\"{var_names[i]}{nums}.tif\")) as handle:\n",
    "                raster_arr = handle.read(1)\n",
    "            for l in np.unique(seg):\n",
    "                m = np.mean(raster_arr[seg == l])\n",
    "                tmp_data.append(m)\n",
    "        data.append(tmp_data)\n",
    "        dates.append(dtime)\n",
    "    except:\n",
    "        pbar.set_description(f\"Don't have all variables for {dtime}\")\n",
    "data = np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(dates)\n",
    "data_s = data[inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/ScoutJarman/Code/ILWA/data/other/usgs_data.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=['datetime'])\n",
    "df = df[df['datetime'].isin(dates)]\n",
    "df = df[['datetime', '309486_00062_00003']]\n",
    "df = df.rename(columns={'309486_00062_00003': 'Elevation', 'datetime': 'date'})\n",
    "\n",
    "# column_names = [f\"ppt_{i}\" for i in np.unique(seg)] + [f\"tmean_{i}\" for i in np.unique(seg)]\n",
    "column_names = []\n",
    "for v_nam in var_names:\n",
    "    column_names += [f\"{v_nam}_{i}\" for i in np.unique(seg)]\n",
    "data_df = pd.DataFrame(data_s, columns=column_names)\n",
    "df.index = data_df.index\n",
    "\n",
    "df_daily = pd.concat([df, data_df], axis=1)\n",
    "\n",
    "df_daily.to_csv(\"/home/ScoutJarman/Code/ILWA/data/other/Daily.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_daily['date'], df_daily['Elevation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_daily.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_daily['swe_1'][:2*365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCA Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raster meta data information\n",
    "def format_data(max_hist=datetime(2021, 9, 30), min_date=datetime(1981, 10, 1)):\n",
    "    raster_path = \"/home/ScoutJarman/Code/ILWA/data/rasters/PRISM/ppt_81-23/ppt19810101.tif\"\n",
    "    with rasterio.open(raster_path) as ref_handle:\n",
    "        for r in [\"r1\", \"r2\", \"r3\"]:\n",
    "            folders_in = [f\"/home/ScoutJarman/Code/ILWA/data/rasters/LOCA/ppt_{r}\",\n",
    "                        f\"/home/ScoutJarman/Code/ILWA/data/rasters/LOCA/tmin_{r}\",\n",
    "                        f\"/home/ScoutJarman/Code/ILWA/data/rasters/LOCA/tmax_{r}\"]\n",
    "            var_names = ['ppt', 'tmin', 'tmax']\n",
    "            # Load in variables\n",
    "            data = []\n",
    "            dates = []\n",
    "            pbar =  tqdm(os.listdir(folders_in[0]))\n",
    "            for file in pbar:\n",
    "                year = int(file[3:7])\n",
    "                month = int(file[7:9])\n",
    "                day = int(file[9:11])\n",
    "                nums = file[3:-4]\n",
    "                dtime = datetime(year, month, day)\n",
    "\n",
    "                raster_arr = np.zeros_like(ref_handle.read(1))\n",
    "                try:\n",
    "                    tmp_data = []\n",
    "                    for i, f_in in enumerate(folders_in):\n",
    "                        with rasterio.open(os.path.join(f_in, f\"{var_names[i]}{nums}.tif\")) as handle:\n",
    "                            transform_larger, _, __ = calculate_default_transform(\n",
    "                                    handle.crs, ref_handle.crs, handle.width, handle.height,\n",
    "                                    *handle.bounds, resolution=ref_handle.res)\n",
    "                            # Perform resampling\n",
    "                            reproject(\n",
    "                                source=handle.read(1),\n",
    "                                destination=raster_arr,\n",
    "                                src_transform=handle.transform,\n",
    "                                src_crs=handle.crs,\n",
    "                                dst_transform=transform_larger,\n",
    "                                dst_crs=ref_handle.crs,\n",
    "                                resampling=Resampling.bilinear)\n",
    "\n",
    "                        for l in np.unique(seg):\n",
    "                            m = np.mean(raster_arr[seg == l])\n",
    "                            tmp_data.append(m)\n",
    "                    data.append(tmp_data)\n",
    "                    dates.append(dtime)\n",
    "                except:\n",
    "                    pbar.set_description(f\"Don't have all variables for {dtime}\")\n",
    "\n",
    "            # Sorts the data by date\n",
    "            data = np.asarray(data)\n",
    "            dates = np.asarray(dates)\n",
    "            # Sort data\n",
    "            inds = np.argsort(dates)\n",
    "            data_s = data[inds]\n",
    "            dates = dates[inds]\n",
    "\n",
    "            # Make the historical dataframe\n",
    "            column_names = []\n",
    "            for v_nam in var_names:\n",
    "                column_names += [f\"{v_nam}_{i}\" for i in np.unique(seg)]\n",
    "            hist_df = pd.DataFrame(data_s, columns=column_names)\n",
    "            hist_dates = pd.DataFrame({'date': dates})\n",
    "            hist_df = pd.concat([hist_dates, hist_df], axis=1)\n",
    "            hist_df.to_csv(f\"/home/ScoutJarman/Code/ILWA/data/other/Daily_{r}.csv\", index=False)\n",
    "            \n",
    "            # # Make the future dataframe\n",
    "            # column_names = []\n",
    "            # for v_nam in var_names:\n",
    "            #     column_names += [f\"{v_nam}_{i}\" for i in np.unique(seg)]\n",
    "            # futu_df = pd.DataFrame(data_s[np.invert(inds)], columns=column_names)\n",
    "            # futu_dates = pd.DataFrame({'date': dates[np.invert(inds)]})\n",
    "            # futu_df = pd.concat([futu_dates, futu_df], axis=1)\n",
    "            # futu_df.to_csv(f\"/home/ScoutJarman/Code/ILWA/data/other/Daily_futu{r}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dates = format_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILWA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
